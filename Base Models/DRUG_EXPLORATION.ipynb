{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1X85ZEjTkNd",
        "outputId": "d47d962f-c9e5-4315-87c3-746216ac46fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhWOvi6vJtqE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Embedding, GlobalMaxPooling1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9-2A6EQAV9f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "train_file_path = '/content/drive/My Drive/drugsComTrain_raw.csv'\n",
        "test_file_path = '/content/drive/My Drive/drugsComTest_raw.csv'\n",
        "train_data = pd.read_csv(train_file_path)\n",
        "test_data = pd.read_csv(test_file_path)\n",
        "data = pd.concat([train_data, test_data])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "data['review'] = data['review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "b2-vXeITJFC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXfXV4R2AeDd"
      },
      "outputs": [],
      "source": [
        "# Fill missing values\n",
        "data = data.fillna('')\n",
        "\n",
        "# Encode the labels\n",
        "le_drug = LabelEncoder()\n",
        "data['drugName'] = le_drug.fit_transform(data['drugName'])\n",
        "\n",
        "le_condition = LabelEncoder()\n",
        "data['condition'] = le_condition.fit_transform(data['condition'])\n",
        "\n",
        "# Split the data\n",
        "X = data['review']\n",
        "y = data[['drugName', 'condition']]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poqfkZcKJ0Vx"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "# Initialize the vectorizers\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "count_vectorizer = CountVectorizer(max_features=5000)\n",
        "\n",
        "# Fit and transform the data\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "cv_train = count_vectorizer.fit_transform(X_train)\n",
        "cv_test = count_vectorizer.transform(X_test)\n",
        "\n",
        "# Combine the features\n",
        "X_train_combined = hstack([tfidf_train, cv_train])\n",
        "X_test_combined = hstack([tfidf_test, cv_test])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Embedding, GlobalMaxPooling1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 500\n",
        "X_train_padded = pad_sequences(X_train_seq, maxlen=max_length)\n",
        "X_test_padded = pad_sequences(X_test_seq, maxlen=max_length)\n",
        "\n",
        "# Create the CNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=128, input_length=max_length),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.5),\n",
        "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "    Dropout(0.5),\n",
        "    GlobalMaxPooling1D(),  # Replaces Flatten\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(y_train.shape[1], activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_padded, y_train, epochs=5, batch_size=32, validation_data=(X_test_padded, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz7u5_VKBTHp",
        "outputId": "b9975be8-054a-4438-b16e-1f803bf5ea5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "5377/5377 [==============================] - 114s 21ms/step - loss: 34413901512704.0000 - accuracy: 0.9006 - val_loss: 64952391958528.0000 - val_accuracy: 0.9055\n",
            "Epoch 2/5\n",
            "5377/5377 [==============================] - 110s 20ms/step - loss: 770518206644224.0000 - accuracy: 0.9041 - val_loss: 788526534754304.0000 - val_accuracy: 0.9055\n",
            "Epoch 3/5\n",
            "5377/5377 [==============================] - 108s 20ms/step - loss: 4367002487488512.0000 - accuracy: 0.9041 - val_loss: 3314435120693248.0000 - val_accuracy: 0.9055\n",
            "Epoch 4/5\n",
            "5377/5377 [==============================] - 108s 20ms/step - loss: 14858829128794112.0000 - accuracy: 0.9041 - val_loss: 10103794599723008.0000 - val_accuracy: 0.9055\n",
            "Epoch 5/5\n",
            "5377/5377 [==============================] - 109s 20ms/step - loss: 38534021377949696.0000 - accuracy: 0.9041 - val_loss: 24444602354237440.0000 - val_accuracy: 0.9055\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fa6a78b3520>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsxRcVbqKAz6"
      },
      "outputs": [],
      "source": [
        "def get_reviews_and_conditions(drug_name):\n",
        "    try:\n",
        "        # Transform the drug name to its encoded form\n",
        "        drug_name_encoded = le_drug.transform([drug_name])[0]\n",
        "\n",
        "        # Filter the dataset for the given drug name\n",
        "        filtered_data = data[data['drugName'] == drug_name_encoded]\n",
        "\n",
        "        # Get the first 5 unique reviews and their associated conditions\n",
        "        unique_conditions = filtered_data['condition'].unique()[:5]\n",
        "        reviews = filtered_data['review'].values[:5]\n",
        "\n",
        "        # Inverse transform the condition codes to their original names\n",
        "        conditions = le_condition.inverse_transform(unique_conditions)\n",
        "\n",
        "        return conditions, reviews\n",
        "    except ValueError:\n",
        "        return \"Drug name not found in the dataset\", []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxgaVOu0NRGd"
      },
      "outputs": [],
      "source": [
        "# Simulate user input\n",
        "user_input = 'Aspirin'\n",
        "conditions, reviews = get_reviews_and_conditions(user_input)\n",
        "print(f\"Conditions associated with {user_input}: {conditions}\")\n",
        "print(f\"Reviews for {user_input}: {reviews}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjaqvNNcMtaE"
      },
      "outputs": [],
      "source": [
        "# Simulate user input\n",
        "user_input = input()\n",
        "conditions, reviews = get_reviews_and_conditions(user_input)\n",
        "print(f\"Conditions associated with {user_input}: {conditions}\")\n",
        "print(f\"Reviews for {user_input}: {reviews}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fkSz9unKzKR"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "model.save('/content/drive/My Drive/drug/drug_exploration.h5')\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('/content/drive/My Drive/drug/tokenizer_explore.pkl', 'wb') as file:\n",
        "    joblib.dump(tokenizer, file)\n",
        "\n",
        "# Save the label encoders\n",
        "with open('/content/drive/My Drive/drug/le_drug.pkl', 'wb') as file:\n",
        "    joblib.dump(le_drug, file)\n",
        "\n",
        "with open('/content/drive/My Drive/drug/le_condition.pkl', 'wb') as file:\n",
        "    joblib.dump(le_condition, file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GOKEU5HTiKmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}